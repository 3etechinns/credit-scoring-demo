{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Scoring Model Using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Scoring\n",
    "A credit score is a numerical expression based on a level analysis of a person's credit files, to represent the creditworthiness of an individual. Traditionally, a credit score was primarily based on credit report information typically sourced from credit bureaus. However, with the proliferation of data science, institutions of any size can develop their own credit scoring system and sharpen them for applications to their target markets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn\n",
    "Scikit-learn  is an Open source machine learning library Python, and features various classification, regression and clustering algorithms, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "\n",
    "Scikit-learn prides itself on the following aspects\n",
    "- Simple and efficient tools for data mining and data analysis\n",
    "- Accessible to everybody, and reusable in various contexts\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Open source, commercially usable - BSD license\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lending Club Dataset\n",
    "\n",
    "Lending Club is a US peer-to-peer lending company, headquartered in San Francisco, California. Lending Club is the world's largest peer-to-peer lending platform. The company states that $15.98 billion in loans had been originated through its platform up to 31 December 2015.\n",
    "\n",
    "We shall use their loan data for this exercise. Their public datasets can be accessed from https://www.lendingclub.com/info/download-data.action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let’s Get Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sklearn\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1\n",
    "Let’s first load the loans data. For your conviniebce, we have made this available as exp-lending-club-data.csv on rorocloud. We shall use a pandas dataframe to read this data into, as pandas is an excellent tool for data preparation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "IN_DATAFILE='https://s3.amazonaws.com/rorodata-datasets/lending-club-data.csv'\n",
    "loans = pd.read_csv(IN_DATAFILE, infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  \n",
    "The dataframe loans has 68 columns of data. However, we shall work with only those columns that we find relevant for our model. We have picked 18 columns for this exercise, these have been identified in the list named features. We want to use these features to predict the column bad_loan (1 for bad loan, 0 for otherwise)- hence, this is a binary classification problem\n",
    "To create this data subset in pandas is a cinch\n",
    "> clean_data=loans[features+[response]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['grade',                     # grade of the loan (categorical)\n",
    "            'sub_grade_num',             # sub-grade of the loan as a number from 0 to 1\n",
    "            'short_emp',                 # one year or less of employment\n",
    "            'emp_length_num',            # number of years of employment\n",
    "            'home_ownership',            # home_ownership status: own, mortgage or rent\n",
    "            'dti',                       # debt to income ratio\n",
    "            'purpose',                   # the purpose of the loan\n",
    "            'payment_inc_ratio',         # ratio of the monthly payment to income\n",
    "            'delinq_2yrs',               # number of delinquincies \n",
    "            'delinq_2yrs_zero',          # no delinquincies in last 2 years\n",
    "            'inq_last_6mths',            # number of creditor inquiries in last 6 months\n",
    "            'last_delinq_none',          # has borrower had a delinquincy\n",
    "            'last_major_derog_none',     # has borrower had 90 day or worse rating\n",
    "            'open_acc',                  # number of open credit accounts\n",
    "            'pub_rec',                   # number of derogatory public records\n",
    "            'pub_rec_zero',              # no derogatory public records\n",
    "            'revol_util',                # percent of available credit being used\n",
    "           ]\n",
    "response='bad_loans'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_data=loans[features+[response]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  \n",
    "Now that we have the data set that we need for modelling, we need to apply some preprocessing steps to get the data into a form that scikit-learn algorithms can use. In almost all cases, scikit-learn algorithms have the interface model.fit(X,y), where X and y are numerical matrices representing the features and the target respectively.\n",
    "We will do only one simple preprocessing steps for this tutorial, i.e. convert categorical data into one-hot encoded format. We use the scikit proprocessing model called LabelBinarizer() on only the Categorial columns.\n",
    "At the end of this step, we have a dataset (X,y)  where X and y are numpy arrays  and all columns are numerical values (with categorical variables being one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_cols=['sub_grade_num', 'short_emp', 'emp_length_num','dti', 'payment_inc_ratio', 'delinq_2yrs', \\\n",
    "                'delinq_2yrs_zero', 'inq_last_6mths', 'last_delinq_none', 'last_major_derog_none', 'open_acc',\\\n",
    "                'pub_rec', 'pub_rec_zero','revol_util']\n",
    "\n",
    "categorical_cols=['grade', 'home_ownership', 'purpose']\n",
    "\n",
    "mapper = DataFrameMapper([\n",
    "('grade',sklearn.preprocessing.LabelBinarizer()),\n",
    "('home_ownership', sklearn.preprocessing.LabelBinarizer()),\n",
    "('purpose', sklearn.preprocessing.LabelBinarizer()),\n",
    "        \n",
    "    ])\n",
    "\n",
    "X1=mapper.fit_transform(clean_data)\n",
    "\n",
    "\n",
    "X2=np.array(clean_data[numerical_cols])\n",
    "\n",
    "\n",
    "X = np.hstack((X1,X2)) #Combines X1 and X2 side by side, i.e. stacks them horizontally\n",
    "y=np.array(clean_data['bad_loans'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "The first step in build and testing models is to do a split of train-test data, so that we have unbiased estimates of the model’s error. We keep aside a third of the data for testing purposes, and use 2/3rds for training. This is accomplished very simply in Scikit-learn as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "The actual model building in scikit is tremendously simplified, and follows the same pattern as below\n",
    "\n",
    "\n",
    "```\n",
    "model=scikit_model_class()       initializes  a model of class scikit_model_class\n",
    "model.fit(X_train, y_train)      fits a model on the training data set\n",
    "model.score(X_test, y_test)      evaluates the performance of the model on the test data set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81184643148500657"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_lm = LogisticRegression()\n",
    "log_lm.fit(X_train, y_train)\n",
    "log_lm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Gradient Boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81142616993399419"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grd = GradientBoostingClassifier(n_estimators=100)\n",
    "grd.fit(X_train, y_train)\n",
    "grd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810511483029\n"
     ]
    }
   ],
   "source": [
    "dtree=DecisionTreeClassifier()\n",
    "parameters = {'max_depth':[5, 10, 15, 20, 25, 32]}\n",
    "dtree_gs = GridSearchCV(dtree, parameters, cv=5)\n",
    "dtree_gs.fit(X_train, y_train)\n",
    "print(dtree_gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a Random Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.811154235989\n"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "parameters = {'max_depth':[5, 15], 'n_estimators':[10,30]}\n",
    "rf_gs = GridSearchCV(rf, parameters)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "print(rf_gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: \n",
    "To score new loan applications, we must remember to do the following things\n",
    "1.\tApply the same data columns as we have used in the modelling process\n",
    "2.\tApply the same pre-processing steps, including the same Label Binarizers that were trained on the original training data (i.e. do not refit )\n",
    "3.\tApply the models that were trained on the training dataset (again, do not refit) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's try it out on this sample application\n",
    "a={ 'delinq_2yrs': 0.0,\n",
    " 'delinq_2yrs_zero': 1.0,\n",
    " 'dti': 8.7200000000000006,\n",
    " 'emp_length_num': 0,\n",
    " 'grade': 'F',\n",
    " 'home_ownership': 'RENT',\n",
    " 'inq_last_6mths': 3.0,\n",
    " 'last_delinq_none': 1,\n",
    " 'last_major_derog_none': 1,\n",
    " 'open_acc': 2.0,\n",
    " 'payment_inc_ratio': 4.5,\n",
    " 'pub_rec': 0.0,\n",
    " 'pub_rec_zero': 1.0,\n",
    " 'purpose': 'vacation',\n",
    " 'revol_util': 98.5,\n",
    " 'short_emp': 0,\n",
    " 'sub_grade_num': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preProcess(a):\n",
    "    data=list(a.values())\n",
    "    colz=list(a.keys())\n",
    "    dfx=pd.DataFrame(data=[data], columns=colz)\n",
    "    dfx\n",
    "\n",
    "    XX1=mapper.transform(dfx)\n",
    "    XX2=dfx[numerical_cols]\n",
    "    XX = np.hstack((XX1,XX2))\n",
    "    return XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48829986303342249"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX=preProcess(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accrording to the Logistic Regression model,the probability of loan default is  0.488299863033\n"
     ]
    }
   ],
   "source": [
    "print(\" Accrording to the Logistic Regression model,the probability of loan default is \", log_lm.predict_proba(XX)[:,1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accrording to the Logistic Regression model,the probability of loan default is  0.534399174411\n"
     ]
    }
   ],
   "source": [
    "print(\" Accrording to the Logistic Regression model,the probability of loan default is \", grd.predict_proba(XX)[:,1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
